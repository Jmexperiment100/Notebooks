{"cells":[{"metadata":{"_uuid":"48e297d63f7783f30e196277080afdd61a5a6441"},"cell_type":"markdown","source":"# Transfer learning:\n- Machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\n\n# || Loading Packages"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os, time, random, cv2\nfrom pathlib import Path\nfrom PIL import Image\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\n\nfrom keras.models import Model\nfrom keras.layers import (Conv2D, Input, Dense, Flatten, Dropout, GlobalAveragePooling2D,\n                          Activation, MaxPooling2D, BatchNormalization, Concatenate, ReLU, LeakyReLU)\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"3867ee6dec7ce349951bcbe1c0c7b968aaadf17b"},"cell_type":"markdown","source":"# || Configuration"},{"metadata":{"_uuid":"14e2eac42ad108b8c27a317b614f28607e97a32a","trusted":true},"cell_type":"code","source":"%matplotlib inline\npd.set_option('max_colwidth', 400)\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 16\nt_start = time.time()\n\nDATA_DIR = \"../input/rice-diseases-image-dataset/LabelledRice/Labelled/\"\nCATEGORIES = os.listdir(DATA_DIR)\nNcategories = len(CATEGORIES)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"9bd59d4092fa9c9d0484db67e321abd1b6c36c3d","trusted":true},"cell_type":"code","source":"# ----------------#\n# Hyperparameters #\n# ----------------#\nTarget_shape = (224, 224)\nTest_size = 0.05\nNepochs = 10\nBatch_size = 32\nLearning_rate = 1e-04","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"c7fe1164dd5ad603e02df4f866122cf90787ba4a"},"cell_type":"markdown","source":"# || Data Preparation"},{"metadata":{"_uuid":"062d5a01dc2a4e88c83a38851bb85aee47c61d95","trusted":true},"cell_type":"code","source":"data = []\nfor category_id, category in enumerate(CATEGORIES):\n    for file in os.listdir(os.path.join(DATA_DIR, category)):\n        if file == \".directory\": continue\n        data.append(['{}{}/{}'.format(DATA_DIR, category, file), category_id, category])\ndata = pd.DataFrame(data, columns=['img_path', 'category_id', 'category'])\ndata['category_cat'] = list(to_categorical(data[\"category_id\"], Ncategories))\n\ndata.sample(n=10, random_state=100).head(n=5)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                                                                                          img_path  \\\n2187    ../input/rice-diseases-image-dataset/LabelledRice/Labelled/Healthy/IMG_20190419_135833.jpg   \n2709    ../input/rice-diseases-image-dataset/LabelledRice/Labelled/Healthy/IMG_20190420_200441.jpg   \n1830    ../input/rice-diseases-image-dataset/LabelledRice/Labelled/Healthy/IMG_20190419_172317.jpg   \n452   ../input/rice-diseases-image-dataset/LabelledRice/Labelled/LeafBlast/IMG_20190419_105827.jpg   \n2973  ../input/rice-diseases-image-dataset/LabelledRice/Labelled/BrownSpot/IMG_20190420_190255.jpg   \n\n      category_id   category          category_cat  \n2187            2    Healthy  [0.0, 0.0, 1.0, 0.0]  \n2709            2    Healthy  [0.0, 0.0, 1.0, 0.0]  \n1830            2    Healthy  [0.0, 0.0, 1.0, 0.0]  \n452             0  LeafBlast  [1.0, 0.0, 0.0, 0.0]  \n2973            3  BrownSpot  [0.0, 0.0, 0.0, 1.0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img_path</th>\n      <th>category_id</th>\n      <th>category</th>\n      <th>category_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2187</th>\n      <td>../input/rice-diseases-image-dataset/LabelledRice/Labelled/Healthy/IMG_20190419_135833.jpg</td>\n      <td>2</td>\n      <td>Healthy</td>\n      <td>[0.0, 0.0, 1.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>2709</th>\n      <td>../input/rice-diseases-image-dataset/LabelledRice/Labelled/Healthy/IMG_20190420_200441.jpg</td>\n      <td>2</td>\n      <td>Healthy</td>\n      <td>[0.0, 0.0, 1.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>1830</th>\n      <td>../input/rice-diseases-image-dataset/LabelledRice/Labelled/Healthy/IMG_20190419_172317.jpg</td>\n      <td>2</td>\n      <td>Healthy</td>\n      <td>[0.0, 0.0, 1.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>../input/rice-diseases-image-dataset/LabelledRice/Labelled/LeafBlast/IMG_20190419_105827.jpg</td>\n      <td>0</td>\n      <td>LeafBlast</td>\n      <td>[1.0, 0.0, 0.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>2973</th>\n      <td>../input/rice-diseases-image-dataset/LabelledRice/Labelled/BrownSpot/IMG_20190420_190255.jpg</td>\n      <td>3</td>\n      <td>BrownSpot</td>\n      <td>[0.0, 0.0, 0.0, 1.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"2ae1fdee7f2ab5dcef1c32120c0a8d214e22d799"},"cell_type":"markdown","source":"# || Dividing the Data To Train, Val and Test"},{"metadata":{"_uuid":"f28cd572ecc14fe39f64b2fb40d5940afb28cd9f","trusted":true},"cell_type":"code","source":"train, val = train_test_split(data, stratify=data.category, test_size=Test_size)\ntrain, test = train_test_split(train, stratify=train.category, test_size=Test_size)\nprint(\"Training images: {}\\nValidating images: {}\\nTesting images: {}\".format(len(train), len(val), len(test)))","execution_count":5,"outputs":[{"output_type":"stream","text":"Training images: 3027\nValidating images: 168\nTesting images: 160\n","name":"stdout"}]},{"metadata":{"_uuid":"62b5dedd44538d604f25898305676935c8debbd9"},"cell_type":"markdown","source":"# || Data Augmentation"},{"metadata":{"_uuid":"3f4c4df110da6a4e0c17c9e2abb822bdd2db4a12","trusted":true},"cell_type":"code","source":"# https://imgaug.readthedocs.io/en/latest/source/examples_keypoints.html\n# https://github.com/aleju/imgaug\n# https://imgaug.readthedocs.io/en/latest/source/augmenters.html\nseq = iaa.Sequential([\n    iaa.OneOf([\n        iaa.Fliplr(0.5), # horizontally flip\n        iaa.Flipud(0.5),\n        iaa.Affine(rotate=45)\n    ]),\n    iaa.SomeOf(2, [\n        iaa.AdditiveGaussianNoise(scale=0.2*255),\n        iaa.Add(50, per_channel=True),\n        iaa.Sharpen(alpha=0.2),\n        iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0)),\n        iaa.Noop(),\n        iaa.Dropout(p=(0, 0.2), per_channel=0.5),\n        iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5),\n    ])\n    # More as you want ...\n])\n# seq_det = seq.to_deterministic()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"80af329428cfc29e0164fbfeca7606aeab4db782"},"cell_type":"markdown","source":"# || Data Generator"},{"metadata":{"_uuid":"c1054500e7fd83eac777b2c74bdd8068a9004cdc","trusted":true},"cell_type":"code","source":"def preprocess_input(img):\n    #--- Rescale Image --- Rotate Image --- Resize Image --- Flip Image --- PCA etc.\n    image = cv2.resize(img, Target_shape, interpolation = cv2.INTER_AREA)\n    image = image/255.\n    return(image)\n\ndef get_bach_balenced_idx(df, batch_size, nCategories):\n    k = batch_size // nCategories\n    idx = []\n    for i in range(nCategories):\n        _ = df[df['category_id']==i].sample(n=k).index.to_list()\n        idx.extend(_)\n    return idx\n    \ndef batch_generator(df, batch_size = 16):\n    while True:\n        # Select files (paths/indices) for the batch\n        df.reset_index(drop=True, inplace=True)\n        idx = get_bach_balenced_idx(df, batch_size, 4)\n        \n        batch_paths = df.iloc[list(idx), :][\"img_path\"].values\n        batch_outputs = df.iloc[list(idx), :][\"category_cat\"].values\n        \n        batch_input = []\n        batch_output = [] \n\n        # Read in each input, perform preprocessing and get labels\n        for input_path, output_ in zip(batch_paths, batch_outputs):\n            img = cv2.imread(str(input_path))\n            input_ = preprocess_input(img)\n            batch_input += [ input_ ]\n            batch_output += [ output_ ]\n        # Return a tuple of (input,output) to feed the network\n        batch_x = np.array( batch_input )\n        # Augmentation for batch of images\n        batch_x = seq.augment_images(batch_x)\n        batch_y = np.array( batch_output )\n        \n        yield( batch_x, batch_y )","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"f707095a933cf3b430ae27315742c307d9b45281","trusted":true},"cell_type":"code","source":"# https://keras.io/applications/\nfrom keras.applications import (Xception, VGG16, VGG19, ResNet50, InceptionV3, \n                                InceptionResNetV2, MobileNet, DenseNet121, NASNetMobile)\ndef build_model(ModelName, InputShape = Target_shape):\n    inp = Input((InputShape[0],InputShape[1],3))\n    if ModelName == \"Xception\":\n        base_model = Xception(input_tensor = inp, include_top=False, weights='imagenet')\n    elif ModelName == \"VGG16\":\n        base_model = VGG16(input_tensor = inp, include_top=False, weights='imagenet')\n    elif ModelName == \"VGG19\":\n        base_model = VGG19(input_tensor = inp, include_top=False, weights='imagenet')\n    elif ModelName == \"ResNet50\":\n        base_model = ResNet50(input_tensor = inp, include_top=False, weights='imagenet')\n    elif ModelName == \"InceptionV3\":\n        base_model = InceptionV3(input_tensor = inp, include_top=False, weights='imagenet')\n    elif ModelName == \"InceptionResNetV2\":\n        base_model = InceptionResNetV2(input_tensor = inp, include_top=False, weights='imagenet')\n    elif ModelName == \"MobileNet\":\n        base_model = MobileNet(input_tensor = inp, include_top=False, weights='imagenet')\n    elif ModelName == \"DenseNet121\":\n        base_model = DenseNet121(input_tensor = inp, include_top=False, weights='imagenet')\n    elif ModelName == \"NASNetMobile\":\n        base_model = NASNetMobile(input_tensor = inp, include_top=False, weights='imagenet')\n        \n    # frozen the first .8% layers\n    NtrainableLayers = round(len(base_model.layers)*0.8)\n    for layer in base_model.layers[:NtrainableLayers]:\n        layer.trainable = False\n    for layer in base_model.layers[NtrainableLayers:]:\n        layer.trainable = True\n    \n    x_model = base_model.output\n    x_model = GlobalAveragePooling2D(name='globalaveragepooling2d')(x_model)\n    # x_model = Dense(1024, activation='relu',name='fc1_Dense')(x_model)\n    # x_model = Dropout(0.5, name='dropout_1')(x_model)\n    # x_model = Dense(256, activation='relu',name='fc2_Dense')(x_model)\n    x_model = Dropout(0.1, name='dropout_2')(x_model)\n\n    predictions = Dense(Ncategories, activation='softmax',name='output_layer')(x_model)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    return model","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"6a660580b3bcb878fe58a034df8b68776a663b24"},"cell_type":"markdown","source":"# || Training"},{"metadata":{"_uuid":"ec51d0ba35fde47cd90b39b559ca40550ad34068","trusted":true},"cell_type":"code","source":"# Callbacks for training: Make sure the training works well and doesnt run too long or overfit too much\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n\nMName = (\"Xception\")\nBestModelWeightsPath = \"{}_weights.best.hdf5\".format(MName)\n\ncheckpoint = ModelCheckpoint(\n    BestModelWeightsPath, \n    monitor='val_accuracy', \n    verbose=1, \n    save_best_only=True, \n    mode='max', \n    save_weights_only = True\n)\nreduceLROnPlat = ReduceLROnPlateau(\n    monitor='val_accuracy', \n    factor=0.2, \n    patience=3, \n    verbose=1, \n    mode='max', \n    cooldown=2, \n    min_lr=1e-7\n)\nearly = EarlyStopping(\n    monitor=\"val_accuracy\", \n    mode=\"max\", \n    patience=4\n)\n\nprint(f\"\\n Transfer Learning with: {MName}: \\n\")\n\nmodel = build_model(MName, InputShape = Target_shape)\n\nmodel.compile(optimizer=Adam(lr=Learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n\ntrain_gen = batch_generator(train, Batch_size)\nval_gen = batch_generator(val, Batch_size)\n\nHistory = model.fit_generator(\n    generator= train_gen,\n    steps_per_epoch=train.shape[0]//Batch_size,\n    validation_data=val_gen,\n    validation_steps=val.shape[0]//Batch_size,\n    epochs = Nepochs,\n    callbacks = [checkpoint, early, reduceLROnPlat]\n)\n\nprint(\"\\nFine tuning\")\nfor l in model.layers[:]:\n    l.trainable = True\n    \nmodel.compile(optimizer=SGD(lr=Learning_rate*1e-02, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n\nHistory = model.fit_generator(\n    generator= train_gen,\n    steps_per_epoch=train.shape[0]//Batch_size,\n    validation_data=val_gen,\n    validation_steps=val.shape[0]//Batch_size,\n    epochs=Nepochs*2,\n    callbacks = [checkpoint, early, reduceLROnPlat]\n)","execution_count":null,"outputs":[{"output_type":"stream","text":"\n Transfer Learning with: Xception: \n\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83689472/83683744 [==============================] - 1s 0us/step\nEpoch 1/5\n94/94 [==============================] - 432s 5s/step - loss: 0.5642 - accuracy: 0.7500 - val_loss: 1.6209 - val_accuracy: 0.6734\n\nEpoch 00001: val_accuracy improved from -inf to 0.67344, saving model to Xception_weights.best.hdf5\nEpoch 2/5\n94/94 [==============================] - 408s 4s/step - loss: 0.5622 - accuracy: 0.7500 - val_loss: 1.2191 - val_accuracy: 0.6766\n\nEpoch 00002: val_accuracy improved from 0.67344 to 0.67656, saving model to Xception_weights.best.hdf5\nEpoch 3/5\n16/94 [====>.........................] - ETA: 6:22 - loss: 0.5616 - accuracy: 0.7500","name":"stdout"}]},{"metadata":{"_uuid":"4674ef7d5132d3af0c228d40c53e32cd8c59ecb0","trusted":true},"cell_type":"code","source":"def plot_trainig_history(History):\n    fig, ax = plt.subplots(1, 2, figsize=(20,5))\n    ax[0].set_title('loss')\n    ax[0].plot(History.epoch, History.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(History.epoch, History.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('acc')\n    ax[1].plot(History.epoch, History.history[\"accuracy\"], label=\"Train acc\")\n    ax[1].plot(History.epoch, History.history[\"val_accuracy\"], label=\"Validation acc\")\n    ax[0].legend()\n    ax[1].legend();\n    plt.show()\n\nplot_trainig_history(History)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f98e5aae4ce8b2bfe83b8720e3cc0f02a77c4100"},"cell_type":"markdown","source":"# || Testing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load Best model weights\nmodel.load_weights(BestModelWeightsPath)\n\ndef process_test_data():\n    test_data = []\n    for path in tqdm(test[\"img_path\"].values):\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        img = cv2.resize(img, Target_shape)\n        label = test[test[\"img_path\"] == path][\"category\"].values[0]\n        category_cat = test[test[\"img_path\"] == path][\"category_cat\"].values[0]\n        test_data.append([np.array(img), label, category_cat])\n    return test_data\n\ndef some_function(x):\n    a = np.zeros(x.shape)\n    a[np.argmax(x, axis=0)] = 1\n    return a\n\npredictions = []\ntrue_categories = []\n# Testing Model on Test Data\ntest_data = process_test_data()\nfig =plt.figure(figsize=(20, 12))\nfor num, data in enumerate(test_data[:30]):\n    true_label = data[1]\n    true_category = data[2]\n    img_data = data[0]\n    \n    y = fig.add_subplot(5, 6, num+1)\n    data = np.expand_dims(img_data, axis=0)\n    model_out = model.predict([data])[0]\n    predictions.append(some_function(model_out))\n    true_categories.append(true_category)\n    \n    most_likely_class_index = int(np.argmax(model_out))\n    class_likelihood = model_out[most_likely_class_index]\n\n    # Get the name of the most likely class\n    class_label = CATEGORIES[most_likely_class_index]\n    \n    y.imshow(img_data)\n    plt.title(class_label + \"/\" + true_label)\n    y.axes.get_xaxis().set_visible(False);\n    y.axes.get_yaxis().set_visible(False);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ab6c34824f3905ef6461abd0fec490a5034b36e","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score\naccuracy_score(np.array(predictions), np.array(true_categories))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}