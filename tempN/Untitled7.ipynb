{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout, Convolution2D,MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "#Load the VGG model\n",
    "image_size = 150\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001D2E3F23888> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3CFD408> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3CFD248> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D2E3F1DE08> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3CFBA48> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3F5CA48> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D2E3F6ACC8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3F72FC8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3F7B9C8> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3F81D08> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D2E3F8B348> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3F95348> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3F9AD08> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3FA2EC8> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D2E3FA8B48> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3FB69C8> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3FB9608> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001D2E3FBCB08> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000001D2E3FCFD88> True\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers except the last 11 layers\n",
    "\n",
    "for layer in vgg_conv.layers[:-11]:\n",
    "\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Habiba Hashmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 14,716,740\n",
      "Trainable params: 14,161,412\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    "\n",
    "# Add new layers\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2012 images belonging to 4 classes.\n",
      "Found 672 images belonging to 4 classes.\n",
      "Found 672 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "augmentor = ImageDataGenerator(rescale=1./255,zoom_range=0.2,shear_range=0.2,horizontal_flip=True)\n",
    "train_generator = augmentor.flow_from_directory('D:/rdataset/use/Training',\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            target_size=(150, 150),\n",
    "                                            color_mode=\"rgb\",\n",
    "                                            shuffle=True,\n",
    "                                            batch_size = 32)\n",
    "\n",
    "# initialize the validation generator\n",
    "val_generator = augmentor.flow_from_directory('D:/rdataset/use/Validation',\n",
    "                                         class_mode=\"categorical\",\n",
    "                                         target_size=(150, 150),\n",
    "                                         color_mode=\"rgb\",\n",
    "                                         shuffle=False,\n",
    "                                         batch_size = 32)\n",
    "\n",
    "# initialize the testing generator\n",
    "test_generator = augmentor.flow_from_directory('D:/rdataset/use/Testing',\n",
    "                                     class_mode=\"categorical\",\n",
    "                                     target_size=(150, 150),\n",
    "                                     color_mode=\"rgb\",\n",
    "                                     shuffle=False,\n",
    "                                     batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - ETA: 41:22 - loss: 1.3817 - acc: 0.18 - ETA: 30:41 - loss: 1.3748 - acc: 0.25 - ETA: 29:24 - loss: 1.4044 - acc: 0.26 - ETA: 27:15 - loss: 1.4064 - acc: 0.22 - ETA: 26:05 - loss: 1.4012 - acc: 0.24 - ETA: 24:07 - loss: 1.4071 - acc: 0.26 - ETA: 23:58 - loss: 1.4043 - acc: 0.26 - ETA: 23:46 - loss: 1.4013 - acc: 0.26 - ETA: 23:43 - loss: 1.3985 - acc: 0.28 - ETA: 23:01 - loss: 1.3918 - acc: 0.28 - ETA: 22:59 - loss: 1.4107 - acc: 0.28 - ETA: 24:26 - loss: 1.4065 - acc: 0.28 - ETA: 24:11 - loss: 1.4062 - acc: 0.29 - ETA: 24:24 - loss: 1.4051 - acc: 0.29 - ETA: 23:55 - loss: 1.4044 - acc: 0.29 - ETA: 23:43 - loss: 1.4032 - acc: 0.29 - ETA: 23:13 - loss: 1.4025 - acc: 0.29 - ETA: 23:50 - loss: 1.4016 - acc: 0.28 - ETA: 24:10 - loss: 1.4006 - acc: 0.27 - ETA: 24:05 - loss: 1.3992 - acc: 0.28 - ETA: 23:36 - loss: 1.3939 - acc: 0.29 - ETA: 22:57 - loss: 1.4110 - acc: 0.28 - ETA: 22:36 - loss: 1.4150 - acc: 0.28 - ETA: 22:04 - loss: 1.4173 - acc: 0.28 - ETA: 21:43 - loss: 1.4263 - acc: 0.28 - ETA: 21:50 - loss: 1.4226 - acc: 0.28 - ETA: 21:21 - loss: 1.4230 - acc: 0.28 - ETA: 20:49 - loss: 1.4233 - acc: 0.28 - ETA: 20:28 - loss: 1.4215 - acc: 0.28 - ETA: 20:01 - loss: 1.4217 - acc: 0.28 - ETA: 19:25 - loss: 1.4207 - acc: 0.28 - ETA: 18:55 - loss: 1.4213 - acc: 0.28 - ETA: 18:26 - loss: 1.4202 - acc: 0.28 - ETA: 18:13 - loss: 1.4195 - acc: 0.28 - ETA: 17:58 - loss: 1.4179 - acc: 0.28 - ETA: 17:46 - loss: 1.4179 - acc: 0.27 - ETA: 17:31 - loss: 1.4170 - acc: 0.27 - ETA: 17:11 - loss: 1.4161 - acc: 0.27 - ETA: 16:35 - loss: 1.4154 - acc: 0.27 - ETA: 16:05 - loss: 1.4149 - acc: 0.27 - ETA: 15:43 - loss: 1.4141 - acc: 0.27 - ETA: 15:22 - loss: 1.4130 - acc: 0.27 - ETA: 14:48 - loss: 1.4120 - acc: 0.27 - ETA: 14:12 - loss: 1.4114 - acc: 0.27 - ETA: 13:46 - loss: 1.4119 - acc: 0.27 - ETA: 13:21 - loss: 1.4118 - acc: 0.26 - ETA: 12:44 - loss: 1.4112 - acc: 0.26 - ETA: 12:11 - loss: 1.4107 - acc: 0.26 - ETA: 11:37 - loss: 1.4102 - acc: 0.26 - ETA: 11:03 - loss: 1.4097 - acc: 0.26 - ETA: 10:33 - loss: 1.4093 - acc: 0.26 - ETA: 10:02 - loss: 1.4088 - acc: 0.26 - ETA: 9:27 - loss: 1.4084 - acc: 0.2677 - ETA: 8:52 - loss: 1.4079 - acc: 0.268 - ETA: 8:24 - loss: 1.4075 - acc: 0.268 - ETA: 7:49 - loss: 1.4072 - acc: 0.269 - ETA: 7:14 - loss: 1.4067 - acc: 0.270 - ETA: 6:43 - loss: 1.4064 - acc: 0.271 - ETA: 6:13 - loss: 1.4061 - acc: 0.270 - ETA: 5:38 - loss: 1.4057 - acc: 0.273 - ETA: 5:07 - loss: 1.4052 - acc: 0.274 - ETA: 4:34 - loss: 1.4044 - acc: 0.277 - ETA: 3:59 - loss: 1.4064 - acc: 0.276 - ETA: 3:24 - loss: 1.4062 - acc: 0.277 - ETA: 2:50 - loss: 1.4058 - acc: 0.276 - ETA: 2:17 - loss: 1.4049 - acc: 0.279 - ETA: 1:44 - loss: 1.4044 - acc: 0.279 - ETA: 1:09 - loss: 1.4039 - acc: 0.279 - ETA: 34s - loss: 1.4043 - acc: 0.278 - 4051s 58s/step - loss: 1.4043 - acc: 0.2781 - val_loss: 1.3795 - val_acc: 0.3135\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - ETA: 59:02 - loss: 1.3796 - acc: 0.25 - ETA: 58:01 - loss: 1.3774 - acc: 0.26 - ETA: 53:18 - loss: 1.3801 - acc: 0.29 - ETA: 52:34 - loss: 1.3779 - acc: 0.28 - ETA: 50:00 - loss: 1.3801 - acc: 0.28 - ETA: 53:55 - loss: 1.3770 - acc: 0.30 - ETA: 51:02 - loss: 1.3734 - acc: 0.32 - ETA: 47:58 - loss: 1.3759 - acc: 0.32 - ETA: 47:01 - loss: 1.3834 - acc: 0.31 - ETA: 45:44 - loss: 1.3834 - acc: 0.31 - ETA: 43:07 - loss: 1.3819 - acc: 0.31 - ETA: 40:51 - loss: 1.3790 - acc: 0.31 - ETA: 38:52 - loss: 1.3780 - acc: 0.31 - ETA: 39:32 - loss: 1.3748 - acc: 0.31 - ETA: 38:18 - loss: 1.3725 - acc: 0.31 - ETA: 37:08 - loss: 1.3694 - acc: 0.31 - ETA: 37:25 - loss: 1.3724 - acc: 0.31 - ETA: 36:53 - loss: 1.3746 - acc: 0.31 - ETA: 35:55 - loss: 1.3747 - acc: 0.31 - ETA: 35:49 - loss: 1.3749 - acc: 0.31 - ETA: 34:47 - loss: 1.3756 - acc: 0.31 - ETA: 34:02 - loss: 1.3769 - acc: 0.30 - ETA: 33:02 - loss: 1.3772 - acc: 0.30 - ETA: 32:16 - loss: 1.3768 - acc: 0.30 - ETA: 31:44 - loss: 1.3777 - acc: 0.30 - ETA: 31:25 - loss: 1.3762 - acc: 0.30 - ETA: 30:35 - loss: 1.3745 - acc: 0.30 - ETA: 29:46 - loss: 1.3723 - acc: 0.29 - ETA: 29:03 - loss: 1.3753 - acc: 0.30 - ETA: 28:45 - loss: 1.3790 - acc: 0.30 - ETA: 27:44 - loss: 1.3792 - acc: 0.29 - ETA: 26:39 - loss: 1.3783 - acc: 0.30 - ETA: 25:53 - loss: 1.3772 - acc: 0.30 - ETA: 25:00 - loss: 1.3815 - acc: 0.30 - ETA: 24:11 - loss: 1.3809 - acc: 0.30 - ETA: 23:14 - loss: 1.3789 - acc: 0.30 - ETA: 22:38 - loss: 1.3794 - acc: 0.30 - ETA: 21:51 - loss: 1.3784 - acc: 0.29 - ETA: 20:59 - loss: 1.3775 - acc: 0.30 - ETA: 20:15 - loss: 1.3759 - acc: 0.30 - ETA: 19:28 - loss: 1.3766 - acc: 0.29 - ETA: 18:37 - loss: 1.3791 - acc: 0.29 - ETA: 17:57 - loss: 1.3784 - acc: 0.30 - ETA: 17:16 - loss: 1.3773 - acc: 0.30 - ETA: 16:28 - loss: 1.3779 - acc: 0.29 - ETA: 15:42 - loss: 1.3781 - acc: 0.29 - ETA: 14:58 - loss: 1.3802 - acc: 0.29 - ETA: 14:20 - loss: 1.3807 - acc: 0.29 - ETA: 13:41 - loss: 1.3799 - acc: 0.29 - ETA: 13:05 - loss: 1.3809 - acc: 0.29 - ETA: 12:31 - loss: 1.3805 - acc: 0.29 - ETA: 11:48 - loss: 1.3802 - acc: 0.29 - ETA: 11:08 - loss: 1.3808 - acc: 0.29 - ETA: 10:28 - loss: 1.3798 - acc: 0.29 - ETA: 9:54 - loss: 1.3825 - acc: 0.2943 - ETA: 9:12 - loss: 1.3827 - acc: 0.292 - ETA: 8:30 - loss: 1.3823 - acc: 0.293 - ETA: 7:47 - loss: 1.3815 - acc: 0.295 - ETA: 7:06 - loss: 1.3802 - acc: 0.297 - ETA: 6:28 - loss: 1.3794 - acc: 0.296 - ETA: 5:48 - loss: 1.3791 - acc: 0.296 - ETA: 5:10 - loss: 1.3775 - acc: 0.297 - ETA: 4:32 - loss: 1.3762 - acc: 0.299 - ETA: 3:55 - loss: 1.3755 - acc: 0.300 - ETA: 3:15 - loss: 1.3738 - acc: 0.302 - ETA: 2:36 - loss: 1.3720 - acc: 0.303 - ETA: 1:56 - loss: 1.3746 - acc: 0.303 - ETA: 1:17 - loss: 1.3750 - acc: 0.301 - ETA: 38s - loss: 1.3743 - acc: 0.300 - 3759s 54s/step - loss: 1.3744 - acc: 0.2999 - val_loss: 1.3250 - val_acc: 0.3247\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - ETA: 44:57 - loss: 1.3079 - acc: 0.40 - ETA: 49:34 - loss: 1.3937 - acc: 0.34 - ETA: 48:34 - loss: 1.3920 - acc: 0.28 - ETA: 42:59 - loss: 1.3746 - acc: 0.30 - ETA: 38:38 - loss: 1.3657 - acc: 0.32 - ETA: 35:52 - loss: 1.3408 - acc: 0.33 - ETA: 34:44 - loss: 1.3283 - acc: 0.33 - ETA: 33:49 - loss: 1.3558 - acc: 0.32 - ETA: 32:30 - loss: 1.3638 - acc: 0.32 - ETA: 31:31 - loss: 1.3615 - acc: 0.33 - ETA: 30:18 - loss: 1.3636 - acc: 0.32 - ETA: 29:36 - loss: 1.3627 - acc: 0.32 - ETA: 28:54 - loss: 1.3611 - acc: 0.33 - ETA: 28:06 - loss: 1.3595 - acc: 0.33 - ETA: 27:15 - loss: 1.3687 - acc: 0.32 - ETA: 26:25 - loss: 1.3586 - acc: 0.33 - ETA: 25:52 - loss: 1.3579 - acc: 0.34 - ETA: 25:17 - loss: 1.3584 - acc: 0.34 - ETA: 24:47 - loss: 1.3587 - acc: 0.33 - ETA: 23:55 - loss: 1.3584 - acc: 0.33 - ETA: 23:02 - loss: 1.3617 - acc: 0.32 - ETA: 22:29 - loss: 1.3627 - acc: 0.31 - ETA: 21:51 - loss: 1.3635 - acc: 0.31 - ETA: 21:11 - loss: 1.3651 - acc: 0.31 - ETA: 20:34 - loss: 1.3631 - acc: 0.31 - ETA: 19:57 - loss: 1.3642 - acc: 0.31 - ETA: 19:26 - loss: 1.3609 - acc: 0.31 - ETA: 18:50 - loss: 1.3586 - acc: 0.32 - ETA: 18:24 - loss: 1.3592 - acc: 0.32 - ETA: 17:56 - loss: 1.3568 - acc: 0.32 - ETA: 17:23 - loss: 1.3559 - acc: 0.32 - ETA: 16:54 - loss: 1.3557 - acc: 0.31 - ETA: 16:39 - loss: 1.3569 - acc: 0.31 - ETA: 16:17 - loss: 1.3572 - acc: 0.31 - ETA: 15:53 - loss: 1.3571 - acc: 0.31 - ETA: 15:49 - loss: 1.3568 - acc: 0.31 - ETA: 15:08 - loss: 1.3575 - acc: 0.31 - ETA: 14:28 - loss: 1.3561 - acc: 0.31 - ETA: 13:49 - loss: 1.3546 - acc: 0.32 - ETA: 13:11 - loss: 1.3538 - acc: 0.32 - ETA: 12:35 - loss: 1.3579 - acc: 0.32 - ETA: 11:59 - loss: 1.3603 - acc: 0.32 - ETA: 11:24 - loss: 1.3614 - acc: 0.31 - ETA: 10:51 - loss: 1.3610 - acc: 0.31 - ETA: 10:19 - loss: 1.3600 - acc: 0.32 - ETA: 9:47 - loss: 1.3590 - acc: 0.3220 - ETA: 9:18 - loss: 1.3571 - acc: 0.324 - ETA: 8:48 - loss: 1.3548 - acc: 0.327 - ETA: 8:21 - loss: 1.3504 - acc: 0.331 - ETA: 7:52 - loss: 1.3526 - acc: 0.330 - ETA: 7:25 - loss: 1.3521 - acc: 0.328 - ETA: 6:58 - loss: 1.3517 - acc: 0.328 - ETA: 6:32 - loss: 1.3504 - acc: 0.330 - ETA: 6:05 - loss: 1.3490 - acc: 0.331 - ETA: 5:40 - loss: 1.3490 - acc: 0.332 - ETA: 5:15 - loss: 1.3479 - acc: 0.334 - ETA: 4:50 - loss: 1.3444 - acc: 0.336 - ETA: 4:25 - loss: 1.3463 - acc: 0.334 - ETA: 4:01 - loss: 1.3449 - acc: 0.336 - ETA: 3:38 - loss: 1.3441 - acc: 0.337 - ETA: 3:15 - loss: 1.3526 - acc: 0.336 - ETA: 2:52 - loss: 1.3572 - acc: 0.335 - ETA: 2:30 - loss: 1.3574 - acc: 0.337 - ETA: 2:08 - loss: 1.3569 - acc: 0.340 - ETA: 1:46 - loss: 1.3565 - acc: 0.343 - ETA: 1:24 - loss: 1.3556 - acc: 0.343 - ETA: 1:03 - loss: 1.3549 - acc: 0.344 - ETA: 41s - loss: 1.3535 - acc: 0.346 - ETA: 20s - loss: 1.3522 - acc: 0.34 - 1939s 28s/step - loss: 1.3512 - acc: 0.3478 - val_loss: 1.2461 - val_acc: 0.4316\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - ETA: 15:31 - loss: 1.6591 - acc: 0.34 - ETA: 13:47 - loss: 1.4977 - acc: 0.29 - ETA: 12:59 - loss: 1.4326 - acc: 0.34 - ETA: 13:33 - loss: 1.4187 - acc: 0.32 - ETA: 13:11 - loss: 1.3786 - acc: 0.36 - ETA: 13:42 - loss: 1.3943 - acc: 0.33 - ETA: 13:22 - loss: 1.3759 - acc: 0.34 - ETA: 12:58 - loss: 1.3659 - acc: 0.34 - ETA: 12:33 - loss: 1.3514 - acc: 0.34 - ETA: 12:29 - loss: 1.3386 - acc: 0.35 - ETA: 12:37 - loss: 1.3432 - acc: 0.35 - ETA: 12:25 - loss: 1.3393 - acc: 0.37 - ETA: 12:19 - loss: 1.3272 - acc: 0.37 - ETA: 12:22 - loss: 1.3213 - acc: 0.38 - ETA: 12:03 - loss: 1.3304 - acc: 0.38 - ETA: 11:43 - loss: 1.3284 - acc: 0.38 - ETA: 11:36 - loss: 1.3213 - acc: 0.38 - ETA: 11:25 - loss: 1.3085 - acc: 0.39 - ETA: 11:08 - loss: 1.3061 - acc: 0.40 - ETA: 10:50 - loss: 1.3154 - acc: 0.39 - ETA: 10:41 - loss: 1.3105 - acc: 0.39 - ETA: 10:23 - loss: 1.3126 - acc: 0.39 - ETA: 10:14 - loss: 1.3125 - acc: 0.39 - ETA: 9:56 - loss: 1.3074 - acc: 0.4010 - ETA: 9:40 - loss: 1.2988 - acc: 0.408 - ETA: 9:23 - loss: 1.2998 - acc: 0.407 - ETA: 9:14 - loss: 1.2931 - acc: 0.413 - ETA: 9:05 - loss: 1.2955 - acc: 0.412 - ETA: 8:53 - loss: 1.2910 - acc: 0.415 - ETA: 8:47 - loss: 1.2883 - acc: 0.418 - ETA: 8:32 - loss: 1.2926 - acc: 0.418 - ETA: 8:18 - loss: 1.2918 - acc: 0.418 - ETA: 8:06 - loss: 1.2909 - acc: 0.418 - ETA: 7:51 - loss: 1.2858 - acc: 0.416 - ETA: 7:36 - loss: 1.2870 - acc: 0.419 - ETA: 7:21 - loss: 1.2893 - acc: 0.416 - ETA: 7:07 - loss: 1.2860 - acc: 0.420 - ETA: 6:53 - loss: 1.2862 - acc: 0.421 - ETA: 6:39 - loss: 1.2854 - acc: 0.422 - ETA: 6:24 - loss: 1.2873 - acc: 0.419 - ETA: 6:12 - loss: 1.2867 - acc: 0.416 - ETA: 6:00 - loss: 1.2912 - acc: 0.412 - ETA: 5:47 - loss: 1.2916 - acc: 0.412 - ETA: 5:33 - loss: 1.2935 - acc: 0.411 - ETA: 5:20 - loss: 1.2915 - acc: 0.409 - ETA: 5:07 - loss: 1.2923 - acc: 0.411 - ETA: 4:54 - loss: 1.2901 - acc: 0.412 - ETA: 4:40 - loss: 1.2899 - acc: 0.414 - ETA: 4:27 - loss: 1.2897 - acc: 0.413 - ETA: 4:14 - loss: 1.2891 - acc: 0.414 - ETA: 4:01 - loss: 1.2864 - acc: 0.417 - ETA: 3:49 - loss: 1.2872 - acc: 0.418 - ETA: 3:36 - loss: 1.2849 - acc: 0.420 - ETA: 3:23 - loss: 1.2845 - acc: 0.421 - ETA: 3:11 - loss: 1.2855 - acc: 0.419 - ETA: 2:58 - loss: 1.2881 - acc: 0.418 - ETA: 2:45 - loss: 1.2871 - acc: 0.419 - ETA: 2:32 - loss: 1.2869 - acc: 0.419 - ETA: 2:20 - loss: 1.2842 - acc: 0.421 - ETA: 2:07 - loss: 1.2810 - acc: 0.423 - ETA: 1:54 - loss: 1.2832 - acc: 0.422 - ETA: 1:41 - loss: 1.2931 - acc: 0.419 - ETA: 1:28 - loss: 1.2918 - acc: 0.419 - ETA: 1:16 - loss: 1.2904 - acc: 0.421 - ETA: 1:03 - loss: 1.2866 - acc: 0.424 - ETA: 50s - loss: 1.2905 - acc: 0.426 - ETA: 38s - loss: 1.2904 - acc: 0.42 - ETA: 25s - loss: 1.2893 - acc: 0.42 - ETA: 12s - loss: 1.2897 - acc: 0.42 - 1365s 20s/step - loss: 1.2888 - acc: 0.4259 - val_loss: 1.2024 - val_acc: 0.5103\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - ETA: 13:41 - loss: 1.2414 - acc: 0.40 - ETA: 12:36 - loss: 1.1726 - acc: 0.46 - ETA: 12:16 - loss: 1.1989 - acc: 0.48 - ETA: 11:54 - loss: 1.1928 - acc: 0.48 - ETA: 11:37 - loss: 1.1578 - acc: 0.50 - ETA: 12:12 - loss: 1.1260 - acc: 0.49 - ETA: 11:55 - loss: 1.1124 - acc: 0.51 - ETA: 12:02 - loss: 1.1389 - acc: 0.50 - ETA: 12:00 - loss: 1.1481 - acc: 0.51 - ETA: 12:12 - loss: 1.1885 - acc: 0.49 - ETA: 12:07 - loss: 1.1976 - acc: 0.49 - ETA: 12:01 - loss: 1.2006 - acc: 0.49 - ETA: 11:51 - loss: 1.1913 - acc: 0.50 - ETA: 11:39 - loss: 1.1979 - acc: 0.49 - ETA: 11:22 - loss: 1.1919 - acc: 0.49 - ETA: 11:04 - loss: 1.2077 - acc: 0.49 - ETA: 10:49 - loss: 1.2222 - acc: 0.49 - ETA: 10:47 - loss: 1.2273 - acc: 0.48 - ETA: 10:34 - loss: 1.2288 - acc: 0.48 - ETA: 10:27 - loss: 1.2260 - acc: 0.48 - ETA: 10:22 - loss: 1.2297 - acc: 0.47 - ETA: 10:10 - loss: 1.2228 - acc: 0.48 - ETA: 10:00 - loss: 1.2131 - acc: 0.48 - ETA: 9:43 - loss: 1.2116 - acc: 0.4870 - ETA: 9:27 - loss: 1.2107 - acc: 0.482 - ETA: 9:19 - loss: 1.2097 - acc: 0.478 - ETA: 9:04 - loss: 1.2029 - acc: 0.486 - ETA: 8:54 - loss: 1.2086 - acc: 0.478 - ETA: 8:39 - loss: 1.2097 - acc: 0.480 - ETA: 8:28 - loss: 1.2055 - acc: 0.480 - ETA: 8:14 - loss: 1.2135 - acc: 0.473 - ETA: 7:59 - loss: 1.2126 - acc: 0.476 - ETA: 7:46 - loss: 1.2088 - acc: 0.479 - ETA: 7:32 - loss: 1.2059 - acc: 0.478 - ETA: 7:20 - loss: 1.1975 - acc: 0.482 - ETA: 7:08 - loss: 1.2052 - acc: 0.476 - ETA: 6:56 - loss: 1.2169 - acc: 0.473 - ETA: 6:42 - loss: 1.2160 - acc: 0.471 - ETA: 6:30 - loss: 1.2120 - acc: 0.473 - ETA: 6:17 - loss: 1.2137 - acc: 0.471 - ETA: 6:05 - loss: 1.2149 - acc: 0.469 - ETA: 5:52 - loss: 1.2105 - acc: 0.474 - ETA: 5:39 - loss: 1.2110 - acc: 0.477 - ETA: 5:26 - loss: 1.2124 - acc: 0.476 - ETA: 5:13 - loss: 1.2143 - acc: 0.473 - ETA: 5:02 - loss: 1.2135 - acc: 0.472 - ETA: 4:48 - loss: 1.2111 - acc: 0.475 - ETA: 4:36 - loss: 1.2110 - acc: 0.476 - ETA: 4:23 - loss: 1.2137 - acc: 0.472 - ETA: 4:11 - loss: 1.2149 - acc: 0.469 - ETA: 3:58 - loss: 1.2140 - acc: 0.469 - ETA: 3:45 - loss: 1.2142 - acc: 0.469 - ETA: 3:32 - loss: 1.2110 - acc: 0.471 - ETA: 3:19 - loss: 1.2093 - acc: 0.470 - ETA: 3:06 - loss: 1.2071 - acc: 0.472 - ETA: 2:53 - loss: 1.2126 - acc: 0.469 - ETA: 2:40 - loss: 1.2147 - acc: 0.466 - ETA: 2:28 - loss: 1.2128 - acc: 0.468 - ETA: 2:15 - loss: 1.2109 - acc: 0.471 - ETA: 2:04 - loss: 1.2103 - acc: 0.472 - ETA: 1:51 - loss: 1.2163 - acc: 0.469 - ETA: 1:39 - loss: 1.2170 - acc: 0.466 - ETA: 1:26 - loss: 1.2142 - acc: 0.468 - ETA: 1:14 - loss: 1.2149 - acc: 0.467 - ETA: 1:02 - loss: 1.2143 - acc: 0.467 - ETA: 49s - loss: 1.2143 - acc: 0.467 - ETA: 37s - loss: 1.2124 - acc: 0.46 - ETA: 25s - loss: 1.2084 - acc: 0.47 - ETA: 12s - loss: 1.2074 - acc: 0.47 - 1620s 23s/step - loss: 1.2086 - acc: 0.4716 - val_loss: 1.1584 - val_acc: 0.4829\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=70,\n",
    "                              epochs=5,\n",
    "                              validation_data=test_generator,\n",
    "                              validation_steps=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - ETA: 8:17 - loss: 1.4913 - acc: 0.312 - ETA: 7:11 - loss: 1.3329 - acc: 0.468 - ETA: 5:41 - loss: 1.2672 - acc: 0.531 - ETA: 4:47 - loss: 1.2828 - acc: 0.500 - ETA: 3:52 - loss: 1.2431 - acc: 0.537 - ETA: 3:03 - loss: 1.2310 - acc: 0.531 - ETA: 2:21 - loss: 1.2031 - acc: 0.544 - ETA: 1:33 - loss: 1.1833 - acc: 0.550 - ETA: 46s - loss: 1.1615 - acc: 0.555 - 754s 75s/step - loss: 1.1527 - acc: 0.5531 - val_loss: 1.0952 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.09518, saving model to new_model.h5\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - ETA: 5:00 - loss: 1.6108 - acc: 0.500 - ETA: 3:44 - loss: 1.4780 - acc: 0.484 - ETA: 3:32 - loss: 1.4070 - acc: 0.510 - ETA: 3:15 - loss: 1.3646 - acc: 0.500 - ETA: 2:53 - loss: 1.3212 - acc: 0.500 - ETA: 2:31 - loss: 1.2827 - acc: 0.536 - ETA: 1:45 - loss: 1.2671 - acc: 0.536 - ETA: 1:09 - loss: 1.2557 - acc: 0.531 - ETA: 34s - loss: 1.2554 - acc: 0.531 - 588s 59s/step - loss: 1.2202 - acc: 0.5442 - val_loss: 1.3202 - val_acc: 0.3937\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.09518\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - ETA: 5:04 - loss: 1.1516 - acc: 0.625 - ETA: 3:45 - loss: 1.1528 - acc: 0.531 - ETA: 3:07 - loss: 1.0855 - acc: 0.572 - ETA: 2:43 - loss: 1.1768 - acc: 0.523 - ETA: 2:28 - loss: 1.1961 - acc: 0.537 - ETA: 1:54 - loss: 1.2055 - acc: 0.536 - ETA: 1:27 - loss: 1.2104 - acc: 0.531 - ETA: 59s - loss: 1.1992 - acc: 0.535 - ETA: 30s - loss: 1.1940 - acc: 0.53 - 499s 50s/step - loss: 1.1771 - acc: 0.5344 - val_loss: 1.2696 - val_acc: 0.4750\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.09518\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - ETA: 4:53 - loss: 1.2819 - acc: 0.406 - ETA: 3:43 - loss: 1.3106 - acc: 0.343 - ETA: 3:04 - loss: 1.2692 - acc: 0.416 - ETA: 2:44 - loss: 1.2494 - acc: 0.421 - ETA: 2:23 - loss: 1.2021 - acc: 0.456 - ETA: 2:02 - loss: 1.1409 - acc: 0.484 - ETA: 1:29 - loss: 1.2514 - acc: 0.459 - ETA: 58s - loss: 1.2825 - acc: 0.460 - ETA: 28s - loss: 1.2991 - acc: 0.43 - 487s 49s/step - loss: 1.2791 - acc: 0.4469 - val_loss: 1.1028 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.09518\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - ETA: 3:50 - loss: 1.0675 - acc: 0.531 - ETA: 3:16 - loss: 1.1831 - acc: 0.390 - ETA: 2:52 - loss: 1.2140 - acc: 0.375 - ETA: 2:31 - loss: 1.1932 - acc: 0.414 - ETA: 2:06 - loss: 1.1692 - acc: 0.431 - ETA: 1:35 - loss: 1.1748 - acc: 0.437 - ETA: 1:14 - loss: 1.1704 - acc: 0.459 - ETA: 50s - loss: 1.1687 - acc: 0.449 - ETA: 24s - loss: 1.1815 - acc: 0.45 - 439s 44s/step - loss: 1.1768 - acc: 0.4594 - val_loss: 1.4774 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.09518\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - ETA: 10:14 - loss: 0.9748 - acc: 0.59 - ETA: 7:59 - loss: 1.1326 - acc: 0.5469 - ETA: 6:19 - loss: 1.1198 - acc: 0.583 - ETA: 4:39 - loss: 1.0902 - acc: 0.601 - ETA: 3:40 - loss: 1.0577 - acc: 0.606 - ETA: 2:46 - loss: 1.1005 - acc: 0.572 - ETA: 1:58 - loss: 1.1120 - acc: 0.571 - ETA: 1:17 - loss: 1.1056 - acc: 0.578 - ETA: 38s - loss: 1.0833 - acc: 0.590 - 439s 44s/step - loss: 1.0961 - acc: 0.5875 - val_loss: 1.0513 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.09518 to 1.05127, saving model to new_model.h5\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - ETA: 2:47 - loss: 1.1027 - acc: 0.531 - ETA: 2:17 - loss: 1.0481 - acc: 0.593 - ETA: 1:57 - loss: 1.0794 - acc: 0.541 - ETA: 1:51 - loss: 1.0688 - acc: 0.554 - ETA: 1:36 - loss: 1.0245 - acc: 0.575 - ETA: 1:20 - loss: 1.0593 - acc: 0.562 - ETA: 59s - loss: 1.0887 - acc: 0.562 - ETA: 39s - loss: 1.0764 - acc: 0.56 - ETA: 19s - loss: 1.0650 - acc: 0.56 - 328s 33s/step - loss: 1.1077 - acc: 0.5500 - val_loss: 1.3687 - val_acc: 0.4750\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.05127\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - ETA: 2:26 - loss: 1.2864 - acc: 0.562 - ETA: 2:26 - loss: 1.3222 - acc: 0.453 - ETA: 2:03 - loss: 1.2414 - acc: 0.489 - ETA: 1:42 - loss: 1.1901 - acc: 0.523 - ETA: 1:23 - loss: 1.1972 - acc: 0.518 - ETA: 1:04 - loss: 1.1641 - acc: 0.546 - ETA: 47s - loss: 1.1735 - acc: 0.540 - ETA: 30s - loss: 1.1502 - acc: 0.53 - ETA: 15s - loss: 1.1594 - acc: 0.53 - 268s 27s/step - loss: 1.1623 - acc: 0.5344 - val_loss: 1.0810 - val_acc: 0.5219\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.05127\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - ETA: 3:52 - loss: 1.1112 - acc: 0.531 - ETA: 2:59 - loss: 1.1166 - acc: 0.578 - ETA: 2:20 - loss: 1.0759 - acc: 0.572 - ETA: 1:52 - loss: 1.0202 - acc: 0.593 - ETA: 1:30 - loss: 0.9764 - acc: 0.618 - ETA: 1:11 - loss: 1.0340 - acc: 0.588 - ETA: 51s - loss: 1.0428 - acc: 0.575 - ETA: 34s - loss: 1.0281 - acc: 0.58 - ETA: 17s - loss: 1.0331 - acc: 0.57 - 273s 27s/step - loss: 1.0264 - acc: 0.5812 - val_loss: 1.9429 - val_acc: 0.2562\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.05127\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - ETA: 2:10 - loss: 1.5711 - acc: 0.375 - ETA: 2:05 - loss: 1.4377 - acc: 0.421 - ETA: 2:05 - loss: 1.3352 - acc: 0.458 - ETA: 1:49 - loss: 1.2913 - acc: 0.468 - ETA: 1:32 - loss: 1.2629 - acc: 0.487 - ETA: 1:10 - loss: 1.2248 - acc: 0.505 - ETA: 52s - loss: 1.1743 - acc: 0.517 - ETA: 33s - loss: 1.1447 - acc: 0.52 - ETA: 16s - loss: 1.1378 - acc: 0.52 - 296s 30s/step - loss: 1.1249 - acc: 0.5344 - val_loss: 1.4658 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.05127\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - ETA: 3:38 - loss: 1.0787 - acc: 0.531 - ETA: 2:37 - loss: 1.0203 - acc: 0.562 - ETA: 2:20 - loss: 0.9350 - acc: 0.625 - ETA: 1:49 - loss: 0.8942 - acc: 0.632 - ETA: 1:36 - loss: 0.9068 - acc: 0.637 - ETA: 1:17 - loss: 0.9351 - acc: 0.630 - ETA: 58s - loss: 1.0601 - acc: 0.584 - ETA: 38s - loss: 1.0873 - acc: 0.55 - ETA: 18s - loss: 1.0905 - acc: 0.55 - 246s 25s/step - loss: 1.0894 - acc: 0.5781 - val_loss: 1.1071 - val_acc: 0.4969\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.05127\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - ETA: 1:28 - loss: 1.1671 - acc: 0.562 - ETA: 1:32 - loss: 1.0806 - acc: 0.546 - ETA: 1:24 - loss: 1.0232 - acc: 0.593 - ETA: 1:10 - loss: 1.0172 - acc: 0.585 - ETA: 59s - loss: 1.0159 - acc: 0.575 - ETA: 46s - loss: 1.0621 - acc: 0.56 - ETA: 35s - loss: 1.1543 - acc: 0.54 - ETA: 24s - loss: 1.1586 - acc: 0.52 - ETA: 12s - loss: 1.1658 - acc: 0.52 - 200s 20s/step - loss: 1.1822 - acc: 0.5187 - val_loss: 1.3629 - val_acc: 0.5062\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.05127\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - ETA: 2:48 - loss: 1.0857 - acc: 0.656 - ETA: 2:26 - loss: 1.1488 - acc: 0.562 - ETA: 2:21 - loss: 1.0983 - acc: 0.583 - ETA: 2:01 - loss: 1.0917 - acc: 0.562 - ETA: 1:46 - loss: 1.0546 - acc: 0.581 - ETA: 1:24 - loss: 1.0447 - acc: 0.578 - ETA: 1:02 - loss: 1.0891 - acc: 0.553 - ETA: 40s - loss: 1.0823 - acc: 0.558 - ETA: 19s - loss: 1.0758 - acc: 0.56 - 305s 30s/step - loss: 1.0907 - acc: 0.5531 - val_loss: 0.9613 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.05127 to 0.96130, saving model to new_model.h5\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - ETA: 3:09 - loss: 1.0353 - acc: 0.687 - ETA: 2:37 - loss: 1.1015 - acc: 0.562 - ETA: 2:16 - loss: 1.0960 - acc: 0.531 - ETA: 1:58 - loss: 1.0442 - acc: 0.546 - ETA: 1:41 - loss: 1.0325 - acc: 0.550 - ETA: 1:28 - loss: 1.0337 - acc: 0.567 - ETA: 1:05 - loss: 1.0437 - acc: 0.584 - ETA: 42s - loss: 1.0512 - acc: 0.570 - ETA: 21s - loss: 1.0822 - acc: 0.56 - 359s 36s/step - loss: 1.0897 - acc: 0.5594 - val_loss: 1.3178 - val_acc: 0.4188\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.96130\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - ETA: 3:35 - loss: 0.9920 - acc: 0.531 - ETA: 3:39 - loss: 0.9377 - acc: 0.656 - ETA: 2:51 - loss: 0.8982 - acc: 0.651 - ETA: 2:12 - loss: 0.9411 - acc: 0.637 - ETA: 1:49 - loss: 0.9126 - acc: 0.641 - ETA: 1:26 - loss: 0.9130 - acc: 0.622 - ETA: 1:02 - loss: 0.9329 - acc: 0.618 - ETA: 40s - loss: 0.9938 - acc: 0.596 - ETA: 19s - loss: 0.9921 - acc: 0.60 - 318s 32s/step - loss: 0.9780 - acc: 0.6111 - val_loss: 1.3247 - val_acc: 0.3969\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.96130\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - ETA: 2:58 - loss: 1.1770 - acc: 0.562 - ETA: 2:16 - loss: 1.4270 - acc: 0.421 - ETA: 2:06 - loss: 1.2877 - acc: 0.489 - ETA: 1:53 - loss: 1.2326 - acc: 0.531 - ETA: 1:40 - loss: 1.1769 - acc: 0.537 - ETA: 1:21 - loss: 1.2049 - acc: 0.546 - ETA: 1:00 - loss: 1.1829 - acc: 0.549 - ETA: 39s - loss: 1.1793 - acc: 0.554 - ETA: 19s - loss: 1.1608 - acc: 0.55 - 295s 29s/step - loss: 1.1509 - acc: 0.5594 - val_loss: 1.3271 - val_acc: 0.4437\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.96130\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - ETA: 2:06 - loss: 1.2882 - acc: 0.593 - ETA: 2:13 - loss: 1.3445 - acc: 0.578 - ETA: 2:21 - loss: 1.2664 - acc: 0.614 - ETA: 1:57 - loss: 1.2215 - acc: 0.593 - ETA: 1:35 - loss: 1.2069 - acc: 0.581 - ETA: 1:14 - loss: 1.2318 - acc: 0.557 - ETA: 55s - loss: 1.2136 - acc: 0.549 - ETA: 37s - loss: 1.1896 - acc: 0.56 - ETA: 18s - loss: 1.1668 - acc: 0.55 - 314s 31s/step - loss: 1.1233 - acc: 0.5844 - val_loss: 1.2327 - val_acc: 0.4844\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.96130\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - ETA: 2:55 - loss: 0.8829 - acc: 0.687 - ETA: 2:35 - loss: 0.9309 - acc: 0.656 - ETA: 2:34 - loss: 0.9793 - acc: 0.604 - ETA: 2:16 - loss: 0.9733 - acc: 0.585 - ETA: 1:57 - loss: 0.9764 - acc: 0.600 - ETA: 1:28 - loss: 0.9989 - acc: 0.588 - ETA: 1:08 - loss: 1.0350 - acc: 0.567 - ETA: 45s - loss: 1.0386 - acc: 0.558 - ETA: 22s - loss: 1.0146 - acc: 0.56 - 330s 33s/step - loss: 1.0001 - acc: 0.5844 - val_loss: 2.5257 - val_acc: 0.3469\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.96130\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - ETA: 3:29 - loss: 0.9366 - acc: 0.687 - ETA: 3:29 - loss: 0.9308 - acc: 0.640 - ETA: 2:43 - loss: 0.8936 - acc: 0.656 - ETA: 2:12 - loss: 0.9260 - acc: 0.656 - ETA: 1:44 - loss: 0.9937 - acc: 0.631 - ETA: 1:24 - loss: 1.0217 - acc: 0.614 - ETA: 59s - loss: 0.9903 - acc: 0.625 - ETA: 38s - loss: 0.9729 - acc: 0.61 - ETA: 19s - loss: 0.9835 - acc: 0.60 - 300s 30s/step - loss: 1.0039 - acc: 0.5969 - val_loss: 1.2544 - val_acc: 0.3906\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.96130\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - ETA: 2:35 - loss: 1.1276 - acc: 0.468 - ETA: 2:47 - loss: 1.0343 - acc: 0.578 - ETA: 2:35 - loss: 1.0521 - acc: 0.572 - ETA: 2:06 - loss: 0.9696 - acc: 0.617 - ETA: 1:40 - loss: 0.9596 - acc: 0.618 - ETA: 1:19 - loss: 0.9652 - acc: 0.619 - ETA: 58s - loss: 0.9694 - acc: 0.625 - ETA: 40s - loss: 0.9888 - acc: 0.60 - ETA: 19s - loss: 0.9645 - acc: 0.61 - 303s 30s/step - loss: 0.9836 - acc: 0.5969 - val_loss: 2.1736 - val_acc: 0.2031\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.96130\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=10,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=10,\n",
    "                              callbacks=[checkpoint]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('new_model.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
